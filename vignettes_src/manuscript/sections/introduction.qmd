
## Introduction {#introduction}

As pharmacology experiments increase in complexity, it becomes increasingly challenging to analyze them. Although various frameworks exist for fitting pharmacology models using maximum likelihood estimation, such as GraphPad `Prism`[@GraphPad_Software_LLC2023-ug] and the `drc` dose-response curves R package[@Ritz2015-dq], it is much less accessible to fit pharmacology models using Bayesian statistics, which is a principled approach to quantify model uncertainty before and after data collection. In practice, Bayesian modeling requires defining a functional form, a prior distribution over the model parameters, and a likelihood function that describes the data generation process. Then, using Markov chain Monte Carlo (MCMC) or variational inference, the prior and the likelihood are combined to generate samples from the posterior distribution over the parameters. These samples can be used directly to answer scientific questions like "what are the model parameters" and "how confident should we be?" Crucially, Bayesian modeling workflows enable incremental addition of complexity to allow for the capture of important signal in the data generation process.  

There has been substantial progress in general computational frameworks to facilitate developing and applying Bayesian models. A key example is the `Stan` package and the ecosystem of supporting tools. `Stan` provides a domain-specific language to describe probabilistic models, inference engines, front-end interfaces through many common programming languages, and a suite of tools to analyze fit models[@Carpenter2017-es; @Burkner2017-eu; @Vehtari2017-vr; @Gabry2017-ff; @Kay2018-cs; @Wickham2009-dl; @Wickham2019-zn; @Team2013-gf]. Among the front-end interfaces, the Bayesian Regression Modeling using Stan (BRMS) package in R facilitates rapid model development through formulas similar to base regression models in R like `lm()` and the mixed effects regression package `lme4`[@Bates2015-dg]. BRMS not only supports defining linear and nonlinear predictors, but also hierarchical models and a range of pre-specified or custom response functions. Once specified as formulas, `brms` translates them into the `Stan` modeling language, where they are compiled, run, and can then be analyzed.

In pharmacology, dose-response modeling through the foundational sigmoidal Hill-equation, Michaelis Menten enzyme kinetics, and multi-drug synergy models are widely used to probe biological systems and develop therapeutics. A limitation of the current tools is that it is currently not straightforward for practitioners to implement and analyze these types of models using Bayesian statistics. To fill this gap, we have developed the `BayesPharma` R package, a Bayesian pharmacology modeling framework that implements foundational pharmacology models so that they can be easily used through `brms`, `Stan`, and other tools in the `Stan` ecosystem. After reviewing related work and basic Bayesian modeling concepts, we describe the package architecture and demonstrate the utility through several case studies.

### Related work {#related}

For general Bayesian modeling theory, there are many excellent textbooks[@Gelman2013-ce; @McElreath2016-zj; @Gelman2006-xa; @Johnson2022-rd], online resources[@Betancourt2023-hn; @Posit_Software_PBC2023-it; @Herbert_Lee2023-fc] and prescriptive guidance[@Depaoli2017-ga; @Kruschke2021-pp; @Gelman2020-ab]. While in theory, Bayesian modeling relies on relatively straightforward statistical principles discussed in the following section, in practice, it is often impossible to fit models analytically. Instead, fitting Bayesian models typically requires using computational simulations or approximations. While bespoke inference methods can be implemented from scratch, for practitioners, it is useful to build on computational frameworks that support model specification, work with probability distributions and samples, and implement algorithms to conduct simulation and variational based inference. Frameworks differ by the language ecosystem they build on, how tightly the components are coupled, and the maturity of the framework, including support for diverse models and analyses and practitioner support such as documentation and usage guides. Beyond the `Stan` framework, which is written in C++ and has command line (`cmdstanr`), R (`stanr`) and, python (`PyStan`) interfaces, the `JAGS` (Just another Gibbs Sampler)  framework is also implemented in C++ and has an R front end with a domain specific language and inference engine[@Plummer2003-ap]. Historically both `Stan` and `JAGS` grew out of the `BUGS` (Bayesian Inference using Gibbs Sampling) that was originally developed in the late 1980s[@Spiegelhalter1996-zm]. In contrast, `PyMC` is implemented in C++ with a Python API and a range of modules for Bayesian inference[@Salvatier2016-sf]. `Pyro` is built on Pytoch[@Paszke2019-tt] and JAX[@James_Bradbury2018-rj] and wraps arbitrary Python code as a probabilistic model, and has an emphasis on deep-learning based variational inference. Turing is implemented in Julia, leveraging the expressive type system to support specifying probabilistic models and performing inference[@Bezanson2017-ui]. Broadly, the increase in maturity in Bayesian modeling frameworks is making Bayesian modeling more accessible to practitioners, where it has seen a steady increase in popularity across the social sciences, econometrics, and biostatistics. 

Over the past three decades, Bayesian methods have also become increasingly common in clinical pharmacology[@Ashby2006-yg; @Grieve2007-rc; @Campbell2017-zw; @Yang2019-kj; @Lakshminarayanan2019-fw; @Cooner2019-zi; @Lesaffre2020-hv; @Faya2021-kp; @Ruberg2023-os]. For example, the first approved COVID-19 Pfizer/Biontech vaccine used a Bayesian clinical trial design[@Polack2020-cb; @Senn2022-nn]. While there has been some application of Bayesian modeling for the analysis of high-throughput screening[@Wei2013-iu; @Lock2015-ms; @Shterev2021-qh; @Ma2021-yd; @Tansey2021-fn], and dose response modeling[@Smith2006-ou; @Johnstone2017-ow; @Labelle2019-jg; @Gould2019-ni; @Arezooji2020-ww; @Semenova2021-lv], the models tend to be bespoke and highly sophisticated. In contrast, we aim to lower the barrier of entry by building simple models while maintaining the flexibility needed to incrementally add complexity by building on a mature Bayesian modeling framework.

## Bayesian Modeling Workflow {#workflow}
Bayesian statistics is a principled strategy to fit models to data. The key idea is that before seeing the data, the researcher defines a prior distribution over possible models indexed by model parameters, then the prior is combined with the data through Bayes theorem to produce the posterior distribution over the parameters. Bayes' theorem can be derived from basic facts about probability distributions. While many have encountered examples of mathematical probability distributions, e.g., the normal distribution over all real numbers or binomial distribution for flips of a biased coin, it is worth thinking about what a probability distribution means from a computational perspective. Roughly, they are objects that support two types of operations, (1) it is possible to draw samples from them and (2) given a region of outcome space, we can ask what is the probability of drawing a sample in that region? For a one-dimensional probability distribution, when we draw many samples, we can form a histogram, and once we have generated this, we can ask then questions about the distribution, such as, what fraction had values larger than a given value. For two random variables, we can think of a sample as a scatter plot over the different dimensions.

Here, we will give an informal derivation of Bayes’ theorem. Consider a noisy data generation process with parameters $\theta \in \Theta$ that can generate data $\mbox{D} \in \mathbb{D}$. Assume we have an initial guess of possible parameters represented as a prior distribution $P(\theta)$ and a way to evaluate how likely a dataset is given a candidate set of parameters, which we call the likelihood conditional probability distribution $P(\mbox{D} | \theta)$. We would like to observe a data D and use it to find a better estimate of the parameters, which we call the posterior conditional probability distribution $P(\theta | \mbox{D})$. Consider the joint distribution of $\theta$ and $\mbox{D}$, $P(\theta, \mbox{D})$. A basic fact of probability distributions is that it is possible to factorize joint distributions into conditional distributions in two different ways:
$$
P(\theta | \mbox{D}) P(\mbox{D}) = P(\theta, \mbox{D}) = P(\mbox{D} | \theta) P(\theta)
$$
Dividing through by $P(\mbox{D})$ gives
$$
\begin{aligned}
    P(\theta | D) &= \frac{P(D | \theta)\cdot P(\theta)}{P(D)} \\
    \mbox{POSTERIOR} &= \frac{\mbox{LIKELIHOOD}\cdot\mbox{PRIOR}}{\mbox{EVIDENCE}}
\end{aligned}
$$
Inspecting this equation, we see that the posterior distribution is proportional to the likelihood time of the prior. Since we want the posterior distribution to be a valid probability distribution that must integrate to $1$, we can understand the evidence $P(\mbox{D})$ to be the unique normalization constant needed to make the equation work. While in principle the evidence term can be computed explicitly, it is often intractable. Instead, we can borrow ideas from statistical mechanics and try to sample parameters according to the posterior distribution. If we interpret the negative log of the posterior probability as a potential energy function, we can simulate how a particle would move on the parameter space over time---like a ball rolling over a hilly landscape---and then take snapshots of the trajectory as our samples. Given enough time, if the particle can access all parts of the probability distribution, one can show that the snapshots will form an unbiased sample from the posterior distribution. The key insight that makes this strategy useful is that evaluating the local change in energy (which determines a particle's trajectory) does not require computing the global absolute energy, and therefore sidesteps computing the intractable evidence normalization constant. Different algorithms have been implemented to do this type of sampling, including Markov Chain based samplers such as Metopolis Hastings Monte Carlo[@Metropolis1953-td, @Hastings1970-yj], Gibbs Sampling[@Geman1984-mn], and Hamiltonian Monte Carlo[@Duane1987-ff, @Neal1996-kd], which considers the energy gradient. Stan uses a variant of Hamiltonian Monte Carlo called No-U-Turn Sampling (NUTs)[@Hoffman2014-pd], which aims to dampen unproductive oscillations to accelerate sampling. An alternative sampling strategy, called variational inference, aims to simplify the computation by defining a more tractable functional form for the posterior and optimizes it to estimate an evidence lower bound (ELBO)[@Kucukelbir2015-wv; @Blei2017-xl]. Depending on the complexity of the functional form, variational inference can be more effective than sampling-based strategies, though typically it is both faster and less accurate.

There are a few key takeaways from understanding the derivation of Bayes’ theorem. First, the posterior distribution—the thing we are trying to estimate in Bayesian statistics—is a distribution over the parameters, not a distribution over, e.g., sampled data. Second, for inference, we combine the prior and the likelihood in a sampling algorithm to generate samples from the posterior distribution. Third, sampling-based inference methods, such as NUTs, that are run for only a finite amount of time are biased by their the initial conditions, while variational inference based methods are biased by the mismatch between the low complexity of the variational family and the high complexity of the true posterior, therefore inference is not guaranteed to work. 

Since Bayesian modeling requires multiple steps, it can be non-trivial for beginners to get started. So, towards using Bayesian modeling effectively in practices, there have been efforts in the Bayesian modeling community to describe a principled Bayesian workflow, such as that described by [@Gelman2020-ab; @Van_de_Schoot2020-er], for building robust Bayesian models and using them to draw inferences. The main steps involve

1. Define and fit a probabilistic model, which combines a *prior* distribution over a set of parameters with the data to draw samples from the *posterior* distribution over the parameters using Hamiltonian Markov chain Monte Carlo.
2. Check for sampling convergence.
3. Use prior and posterior predictive checks to evaluate the model specification and fit.
4. Use cross validation to evaluate the generalizability of the model.
5. Assess inferences that can be made from the model.

### BayesPharma package design {#package_design}
We designed the `BayesPharma` package to support modeling of pharmacology data using the principled Bayesian workflow. As described above, the workflow involves four phases: model specification, model fitting, model evaluation, and interpretation. Here we will describe the `BayesPharma` interface and how we recommend using it. 

To provide data to the `BayesPharma` package, the user provides an R `data.frame` with columns for the response, treatments, and optionally additional covariates such as `drug_id` or `batch_id`. The data is passed to a model function that optionally includes arguments to customize the formula, prior, initial values, and other arguments to control the model fitting. The `BayesPharma` package then passes the user input to `brms::brm()` along with custom `stan` code specific for the selected model. Once the model is fit, the resulting `brms::brmsfit` object is returned to the user and can be used for analysis.


#### Model specification {#model_specification}
Here, we will describe the components that are needed to specify the model.

**Formula**: The goal of the formula is to describe how the data is generated conditional on the model parameters. Syntactically, `BayesPharma` model formulas build a `brms::brmsformula`, which is similar to the formula specification syntax in base R and other R regression modeling packages. A `brms::brmsformula` consists of an equation that declares how the response on the left side is sampled from a parameterized distribution on the right side. For the linear formulas in BRMS, `brms::lf()`, the right side specifies mean response with a linear combination of covariates added together with implicitly defined model parameters. For example, the formula

$$
  \mbox{response} \sim 1 + \mbox{drug\_id}
$$ 
says that `response` is sampled from a distribution with mean $\beta_0 \cdot 1 + \beta_1 \cdot \mbox{drug\_id}_1 + \beta_2 \cdot \mbox{drug\_id}_2 + \dots \beta_n \cdot \mbox{drug\_id}_n$ where $\beta_i$ are scalar parameters and $\mbox{drug\_id}_i$ is an indicator variable for drug $i$. By default, the sampling distribution is a Gaussian, but other distributions can be specified from the distribution family with a link function using the `family` argument. For example, to model count data, which is strictly positive, set `family=brms::poisson()`. To model more general sampling equations, `brms::formula` can be specified as non-linear by setting `nl=TRUE`, and all model parameters must be explicitly defined. Building on this framework, the `brms` package supports a wide range types of regression models including hierarchical models or random effects models. Moreover, it can support observational models that handle, for example, missing data or measurement error, which are described in detail in[@Burkner2017-eu]. The `BayesPharma` package extends the `brms` formula syntax by defining Stan functions for foundational model types, such as the sigmoid function to model Hill-equation dose response models. For each model, functions are provided to help build the formula, for example

\scriptsize
```{r demo-formula}
#| echo = TRUE

# This formula...
demo_formula <- BayesPharma::sigmoid_agonist_formula(
  predictors = 1 + drug_id)

# will generate the equivalent formula as this
demo_formula_alt <- brms::brmsformula(
  response ~ sigmoid(ec50, hill, top, bottom, log_dose),
  nl = TRUE,
  family = brms::student()) +
  brms::lf(ec50 ~ 1 + drug_id) + 
  brms::lf(hill ~ 1 + drug_id) +
  brms::lf(top ~ 1 + drug_id) +
  brms::lf(bottom ~ 1 + drug_id)
```
\normalsize

**Data**: The data to be passed to the model should be in an R `data.frame` and organized into a "tidy" format. This means that each observation is in a single row and there there are columns for the `response`, and model specific covariates such as `log_dose` for the sigmoid model, and additional experimental covariates that can be used as predictors. See table XXX for the required columns (Treatments and Response) for each of the implemented model types

To get data in to a tidy form, it is possible to use the range of tools from the R tidyverse libraries. For example, if data is organized in plate-layout with multiple observations per row, the `tidyr::pivot_longer` function can be used to transform the shape of `data.frame` into one observation per row. Once the data is in a tidy format, the `dplyr` package can be used to manipulate the data, including using `filter` to subset the rows, `select` to subset the columns, `mutate` to compute new columns rowwise, `group_by` with `summarize` and `reframe` to perform split-apply-combine patterns over groups of rows, or `<inner, left, right, full, cross, or semi>_join` to merg using SQL-like semantcs. The `stringr` package can manipulate string data, and for exploratory visualization, the `ggplots2` package implements the grammar-of-graphics workflow for mapping data in a tidy format to aesthetic attributes of geometric objects in the plot, such as the coordinates of points or lines. For a good introduction to data manipulation using the tidyverse, the R for Data Science (2e) book and website (https://r4ds.hadley.nz/) are quite good. Through each of the vigenttes below, we will make use of the tidyverse data manipulation. To illustrate, we will create demonstration data for three drugs C1, C2, C3 with different $\mbox{EC}_{50}$ values of $10\,\mbox{nM}$, $100\,\mbox{nM}$, and $1\,\mu\mbox{M}$ measured and sigmoidal responses.

\scriptsize
```{r demo-data}
#| echo = TRUE
demo_data <- tibble::tibble(
  drug_id = c("C1", "C2", "C3"),
  ec50 = c(-8, -7, -6)) |>
  dplyr::cross_join(
    tidyr::expand_grid(
      log_dose = seq(-7, -5, length.out = 10),
      replica = c(1,2,3))) |>
  dplyr::mutate(
    mean_response = BayesPharma::sigmoid(
      ac50 = ec50,
      hill = 1,
      top = 1,
      bottom = 0,
      log_dose = log_dose),
    response = c(
      stats::rnorm(
        n = dplyr::n(),
        mean = mean_response,
        sd = ifelse(replica == 1, .2, .8))))
      
```
\normalsize
Note that the tidyverse framework—and especially the dplyr package—uses what is called non-standard-evaluation, where column names (e.g., `ec50` and `log_dose` in the `dplyr::mutate()` call) can be treated as variables without needing to put them in quotation marks.


**Prior**: A key principle of Bayesian models is that they require specifying priors. For newcomers, understanding how to determine how these priors should be specified and justified tends to be one of the more challenging parts of the modeling process. From a practical perspective, priors can be thought of as just defining a weighted region of parameter space over which to optimize the model to best fit the data. In particular, the more compact and closely aligned the priors are with the data, the easier it is for the model to fit the data. Therefore, for setting up and getting started with a new model fit, the stronger (more constrained) the priors, the easier the model is to fit. From a scientific perspective, since priors and posterior distributions can be interpreted as capturing the uncertainty in parameters before and after observing the data, the weaker (less constrained) priors, the less biased the inference, "letting the data speak for itself".  Ultimately, however, in Bayesian modeling, it is not possible to completely remove the bias due to the prior. This means that in a complete Bayesian analysis, some substantive argument should be made to justify that inferences from the model are not sensitive to reasonable choices of the prior. In a way, this actually makes choosing the prior less stressful as there is no singular best prior choice, and instead it reflects the scientific questions of the modeling process. For a deeper discussion and practical advice, see: the [prior choice recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) page on the `Stan` wiki. To facilitate specifying priors for each of the `BayesPharma` models, the `BayesPharma` package implements helper functions, for example
  
\scriptsize
```{r demo-prior}
#| echo = TRUE

# This formula
demo_prior <- BayesPharma::sigmoid_agonist_prior()

# will generate the equivalent formula as this
demo_prior_alt <- c(
  brms::prior(prior = normal(-6, 2.5), nlpar = "ec50"),
  brms::prior(prior = normal(1, 1), nlpar = "hill", lb = 0.01),
  brms::prior(prior = normal(1, 0.5), nlpar = "top"),
  brms::prior(prior = normal(0, 0.5), nlpar = "bottom"))
```
\normalsize
The `brms::prior` function takes in `Stan` code (in this case the [Normal distribution](https://mc-stan.org/docs/2_20/functions-reference/normal-distribution.html), which is defined in the `Stan` documentation), the `nlpar` defines the non-linear parameter, and optional arguments `ub` and `lb` give upper or lower bounds. For each `BayesPharma` prior helper function, individual priors can either be explicitly given to override the defaults, or specified as numeric constants to fix them to a particular value.

**Init**: To estimate the posterior distribution using sampling-based inference, the initial parameters values must be given. Then during inference, the parameters are simulated so that they hopefully converge to a sample from the posterior distribution. Therefore, the initial parameter values should at least be in a feasible region of parameter space to get the simulation to rapidly mix. For each model, the `BayesPharma` package provides default initialization values. Typically, if a different prior is given, the initial values can be adjusted along with the updated prior.

\scriptsize
```{r demo-init}
#| echo = TRUE

# This formula
demo_init <- BayesPharma::sigmoid_agonist_init()

# will generate the equivalent initial values as this
demo_init_alt <- function() {
  list(
    b_ec50 = function(){as.array(-6)},
    b_hill = function(){as.array(1)},
    b_top = function(){as.array(1)},
    b_bottom = function(){as.array(0)})
}
```
\normalsize

Note that the `b_` in the function list corresponds to how the parameter names are translated by BRMS from the input formula to the Stan modeling language.

To summarize the different model types implemented in BayesPharma so far,

|Name   |Type           |Treatments                |Parameters                            |Response|
|-------|---------------|--------------------------|--------------------------------------|---------
|Sigmoid|one treatment  |log_dose                  |top, bottom, AC50, hill               |response|
|MuSyC  |two treatments |logd1, logd2              |logE[0-3], logC[1,2], h[1,2], logalpha|response|
|tQ     |enzyme kinetics|series_index, time, ET, ST|Kcat, kM                              |P       |



#### Model fitting {#model_fitting}
Once the components of the model have been specified, to fit the model, each model type provides a function to integrate the formula, data, prior, init and additional arguments to build and fit the model. 

\scriptsize
```{r demo-fit-demo}
#| echo = TRUE,
#| message=FALSE,
#| results='hide',
#| dependson=c("demo-formula", "demo-data", "demo-prior", "demo-init")

# This model ...
demo_model <- BayesPharma::sigmoid_model(
  formula = demo_formula,
  data = demo_data,
  prior = demo_prior,
  init = demo_init)

# is equivalent to:
demo_model_alt <- brms::brm(
  formula = demo_formula,
  data = demo_data,
  prior = demo_prior,
  init = demo_init,
  control = list(adapt_delta = 0.99),
  iter = 8000,
  stanvars = BayesPharma::sigmoid_stanvar())
brms::expose_functions(demo_model_alt, vectorize = TRUE)
demo_model_alt$model_type <- "sigmoid"
```
\normalsize

The call to `brms::expose_functions()` allows calling the compiled model functions in R, which is needed for the downstream analysis described in the next section.

#### Model Evaluation {#model_evaluation}
Following the principled Bayesian workflow, evaluation of the model fit involves evaluating not only the parameter estimates, but also checking inference convergence, checking how sensible the prior and posterior distributions are with the scientific undestanding of the problem, selecting among alternative models, and testing hypotheses, which we will briefly review below here:

**Convergence**: For sampling based inference, while simulations are guaranteed to converge to the posterior eventually, for any finite sample, there is a risk that the samples may be biased by the initial values. To assess convergence, a key strategy is to run multiple chains and compare the within chain correlation against the between chain covariation in parameter estimates. If the chains have not converged, then the between chain covariation will be high. To quantify[@Gelman1992-qk], define $\hat{R}$, where values greater than `1` (e.g., `1.1` and larger) indicate lack of convergence.

To mitigate this bias, there are two key strategies. First, sample the model for longer, or second, reparametrize the model to make it easier to sample. A benefit of the NUTs sampling algorithm used by Stan is that it will report when the sampling has gotten stuck and give warnings.  Michael Betancout has a detailed discussion about these warnings, what they mean, and how to resolve them on his [website](https://betanalpha.github.io/writing/), especially his chapter on [Identifiability and Divergences](https://betanalpha.github.io/assets/case_studies/identifiability.html).

**Prior and Posterior Predictive Checks**: A key strategy in developing and checking the quality of a model is to sample from the prior and posterior distributions and visualize the resulting outcomes. In sampling from the prior, the goal is to make sure that the resulting distributions are consistent with the scientific understanding of the model. For example, if the collected data are counts, but the default Normal distribution family is used, then since the Normal distribution has infinite support, the prior model may generate samples with negative counts, which does not make sense. Seeing these negative counts would motivate using either a family bounded below by zero, or setting a lower bound of zero by setting `lb = 0` in the prior specification.

For posterior predictive distributions, the goal is to generate samples from the posterior and qualitatively evaluate if it is consistent with the observed data. For example, if the observed data is multi-modal, but the posterior samples are unimodal, this may suggest that a different functional form is needed. Using plots and visualizations can reveal these and other issues with the model that may need to be explicitly modeled. Note that in contrast with typical frequentist modelling, the goal is not to construct the simplest meaningful model in order to have the most power to reject it, the goal is to construct a model that best fits the data in order to give the best interpretation of the observations.

**Model Selection**: A general modeling strategy is to begin with simpler models and incrementally increase model complexity to handle nuance as needed. Simpler models are not only easier to interpret, but given a simple model and complex model that explain the data equally, Occam's razor says that the simplier model should be preferred, in part because it is more likely to generalize to unseen contexts. Evaluating model fit while taking into account model complexity is non-trivial for non-Bayesian models and can be done in ad-hoc ways through the number of parameters, deviance scores, etc. However, for Bayesian models, explicitly modeling the model uncertainty gives a principled approach by measuring the marginal likelihood of the data given the model, quantified by expected log-posterior density (ELPD) of held out data. Instead of splitting the data into train/test split, k-fold or leave-one-out (LOO) cross-validation can be used, where the model is fit to a portion of the data and the ELPD is measured on the rest, across multiple partitions. Re-fitting the model for each data-point is computationally intensive, but it can be approximated through Pareto Smoothed Importance Sampling (PSIS) and is implemented in the `loo` package[@Vehtari2017-pw]. By default, the `BayesPharma` package models compute the `loo` criterion. Checking the `model$criteria$loo` will summarize the outlier data points (Pareto k-statistics > 0.7). See [Loo interpretation](https://mc-stan.org/loo/articles/online-only/faq.html#elpd_interpretation) for more guidance in how to interpret the summary statistics. Then, given two models for the same data, calling `brms::loo_compare(model1, model2, ...)`, will rank the models based on their ELPD. See the case-studies for concrete examples. An alternative strategy to compare models is to use Bayesian Model Averaging (BMA), where multiple models are fit separately and then averaged depending on their ability to explain the data. The models that receive weights greater than zero contribute to explaining the overall data distribution.

**Hypothesis Testing**: After models have been fit and selected, the posterior distribution can be used to test hypotheses. For example, to test if one parameter is greater than another, the fraction of samples from the posterior in which the condition is true can be interpreted as a p-value.

Together, these stages of model fitting and evaluation are the foundation for a principled work, that is outlined in more detail in [@Gelman2020-ab]. Understanding what steps need to be communicated depends on the the context, but in our view, the case-studies in the following sections illustrate templates that can be for typical Pharmacology analyses.
