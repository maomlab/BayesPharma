---
title: "BayesPharma: Bayesian methods for pharmacology models"
format:
  jss-pdf:
    keep-tex: true  
  jss-html: default
author:
  - name: Madeline J. Martin
    email: martin.mjm105@gmail.com
    affiliations:
      - ref: knight
  - name: Elayne Vieira Diaz
    email: Elayne.Vieiradias@ucsf.edu
    affiliations:
      - ref: weill
  - name: P. Walter German
    email: p.walter.german@gmail.com
    affiliations:
      - ref: weill
  - name: Elyssa B. Margolis
    email: elyssa.margolis@ucsf.edu
    orcid: 0000-0001-8777-302X
    affiliations:
      - ref: weill
      - ref: ucsf_ngp
  - name: Matthew J. O'Meara
    email: maom@umich.edu
    orcid: 0000-0002-3128-5331
    attributes:
      corresponding: true
    affiliations:
      - ref: dcmb
affiliations:
  - id: knight
    name: University of Oregon
    department: Knight Campus Center for Accelerating Scientific Impact
    city: Eugene
    state: OR
    postal-code: 97403
    country: USA
  - id: weill
    name: University of California, San Francisco
    department: UCSF Weill Institute for Neurosciences, Department of Neurology 
    city: San Francisco
    state: CA
    postal-code: 94158
    country: USA
  - id: ucsf_ngp
    name: University of California, San Francisco
    department: Neuroscience Graduate Program
    city: San Francisco
    state: CA
    country: USA
  - id: dcmb
    name: University of Michigan
    department: Department of Computational Medicine and Bioinformatics
    city: Ann Arbor
    state: MI
    country: USA
abstract: |
  In pharmacology, many experiments seek to measure how a reductive biological
  system responds to one or more treatments. Here we present BayesPharma, a
  collection of Bayesian methods to help analyze these experiments. BayesPharma
  is an \texttt{R} package to facilitate applying a principled Bayesian workflow
  to analyze pharmacology data, built around the \texttt{Stan} ecosystem.
  BayesPharma can be used out of the box to fit and analyze several foundational
  pharmacology models; as a pedagogical framework for learning Bayesian methods;
  and as a starting point for building and analyzing sophisticated
  pharmacological models.  
bibliography: BayesPharma_manuscript_references.bib
editor: visual
header-includes: |
  % https://colorbrewer2.org/#type=qualitative&scheme=Dark2&n=3
  \definecolor{dose}{rgb}{0.10546875,0.6171875,0.46484375}
  \definecolor{response}{rgb}{0.8476562,0.3710938,0.0078125}
  \definecolor{shape}{rgb}{0.4570312,0.4375000,0.6992188}

keywords: [Bayesian, Pharmacology, R]
keywords-formatted: [Bayesian, Pharmacology, "[R]{.proglang}"]
---
```{r set-options}
#| echo=FALSE
# inspired by https://www.jumpingrivers.com/blog/knitr-default-options-settings-hooks/
knitr::opts_chunk$set(
  cache = TRUE,
  echo = FALSE,
  fig.path = "BayesPharma_manuscript/quarto-",
  fig.retina = 2, # Control using dpi
  fig.width = 6,  # generated images
  fig.height = 5, # generated images
  fig.pos = "t",  # pdf mode
  fig.align = "center",
  dpi = if (knitr::is_latex_output()) 72 else 300, 
  out.width = "100%")
#  dev = "svg",
#  dev.args = list(png = list(type = "cairo-png")),
#  optipng = "-o1 -quiet")
# cmdstanr is more up-to-date than rstan and runs a little faster
# but can't expose defined functions for downstream analysis
if(Sys.info()["machine"] == "arm64"){
  #rstan does not currently support apple M1 chips
  stan_backend <- "cmdstanr"
} else {
  stan_backend <- "rstan"
}
```

# Introduction
As pharmacology experiments increase in complexity, it becomes increasingly challenging to analyze them. While there are many frameworks for fitting common curves in pharmacology ranging from the user friendly GraphPad Prism, the \texttt{drc} dose response curves \texttt{R} package\ref{}, to generic \texttt{nlm} packages, a principled approach is to use Bayesian statistics to quantify model uncertainty before and after collecting data. In practice this requires defining a functional form, a prior distribution over the model parameters, and likelihood function that models the data generation process. Then using Markov Chain Monte-Carlo (MCMC) or variational inference the prior and the likelihood are combined to generate samples from the posterior distribution over the parameters. 

Excitingly, there has been rapid progress in developing computational frameworks to facilitate developing and applying Bayesian models. A key example is the Stan ecosystem, which defines a model description language, inference engines, front-end interfaces to many common languages, and a suite of tools to analyze fit models ([@Carpenter2017-pj, @Burkner2017-ww, @Vehtari2017-pw, @Gabry2017-jm, @Kay2018-kn, @Wickham2016-xy, @Wickham2019-jb, @Team2013-nq). To facilitate rapid application model development, the Bayesian Regression Modeling using Stan (BRMS) package in R implements a formula-based interface for Stan similar to \texttt{lme4}, that support defining linear and nonlinear predictors, hierarchical models, and a range of pre-specified or custom response functions. Once specified these models are translated into the Stan modeling language, where they are compiles and run.  

Here we describe the BayesPharma R package, a novel Bayesian pharmacology modeling framework building on the \texttt{Stan} and \texttt{BRMS}. After reviewing basic Bayesian modeling concepts in the context of dose-response experiments, we will describe the package architecture and demonstrate the utility through several case studies.

# Bayesian Modeling Workflow {#workflow}
The scientific method can be conceptualized through building and analyzing models. Consider we are interested in interacting with a complex system such as an rocket ship, ecosystem, person with an illness etc., but we are cautious because doing so may be dangerous, expensive, slow, or unethical. Therefore we may develop a tractable model system to interact with instead (such as a very small rocket ship, zebrafish, or molecular simulation, etc.). If the model system corresponds with to the system of interest in relevant ways, then poking and prodding the model system can be use to anticipate how the system of interest will respond when it is in turn poked and prodded. A key step in establishing this correspondence is to collect a small amount of data from the system of interest and use it design the model system to be representative.

When scientific model systems are constructed by composing conditional probability distributions, they are often called Bayesian models. This is because the key step of going from a nebulous set of possible models to more precise set of models based on observed data uses the celebrated Bayes theorem. While basic Bayesian theory is covered in range of introductory and advanced textbooks. To complement those totally reasonable expositions, here we will correct, but informal derivation of Bayesian probability, to helpfully build intuition and facilitate communication with practitioners. Let $\theta \in \Theta$ be a vector of model parameters and let $\mbox{D} \in \mathbb{D}$ observed data. For concreteness, you can think of $\theta = \left(\mbox{Top}, \mbox{Bottom}, \mbox{IC}_{\mbox{50}}, \mbox{Slope}\right)$ as the parameters for specific sigmoid function, and $\mbox{D}=\{(\mbox{dose}_i, \mbox{response}_i)_{i\in N}\}$, a set of dose-response measurements. A probability distribution naturally models an event or an occurrence. The event is sometimes called a random variable.  Given a region, the chance that the event happens in that region is the probability density over it. Assuming the event definitely happens, then the total density must integrate to unity. A joint distribution over two random variables is just the probability that both events occur.  A conditional probability distribution can be thought of as a stochastic function, where evaluation corresponds to setting the values to be conditioned and obtaining the remaining values. Concretely if you have a scatter plot, you could sample points by first sampling along the x-axis and within the narrow range of the chosen x-value, pick a y-value. Or visa versa, first sample along the y-axis and then within the narrow range of the chose y-value, pick the x-value. Then the joint probability distribution over parameters and data, $P(\theta, D)$ can be written as conditional probability distributions two different ways, $P(\theta | D)P(D)$, and $P(D | \theta)P(\theta)$. Setting them equal to each other and dividing through by $P(D)$ gives Bayes Theorem:
$$
    P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}.
$$
With that in mind, lets look at each term in the is equation. The easiest is the $P(\theta)$, this called the prior must be specified before hand. We'll come back to how to chose priors below. The term $P(D | \theta)$ is called the likelihood, and can thought of as the data generation process, the "model system" if you will. On the left is the $P(\theta | D)$, which is called the posterior. This is a little trickier to understand, but thinking of it as a stochastic function, we can at least inspect the signature: it takes in data, and produces random samples of parameters. Finally the $P(D)$ is called the evidence, and is basically there to make the left and the right be equal to each other. As we'll see below, we don't actually need to evaluate it, but if did we could use a trick called marginalization.

How do we sample parameter values from the posterior distribution? One way to conceptualize problem is to use Boltzmann's equation to convert the probability distribution to an energy function and temperature. Then to sample, begin at a location in parameter space and simulate the kinematic trajectory as it would move through the parameter space over time. Given enough time, if the parameter can access all parts of the probability distribution, the parameter will equilibrate and give sample from the probability distribution. While this is guaranteed to happen eventually, in practice a common concern is that for any fixed amount of equilibration, it is unclear if there has been sufficient mixing. We'll come back to strategies to detect and handle issues with mixing below. A basic kinematic sampling algorithm called Markov Chain Monte Carlo (MCMC) says to take a step, and if it has lower energy, accept, if it has higher energy accept with a probability proportional to the difference in probabilities between the two states and the simulation temperature. A key insight is that because only relative probabilities are needed, the global evidence normalization factors cancel and therefore they don't even need to be computed. More sophisticated algorithms take into account the not only the specific energy values but also the energy gradient. Thinking of the object as having inertia, the sampling allows it to overcome certain types of local minima. to take steps that are likely to be accepted, sometimes called Hamiltonian Monte Carlo. A particularly annoying pat of HMC is that in some cases, such as a narrow ravine in the energy landscape, the trajectory will vibrate back and forth without making much forward progress. A clever heuristic that is used by the Stan, called No U-turn sampling (NUTs), tries to dampen these types of unproductive moves.     

The principled Bayesian workflow [@Gelman2020-sf, @Van_de_Schoot2020-ei], describes a general protocol building robust Bayesian models and using them to draw inferences. The main steps involve
\begin{enumerate}
\item Define and fit a probabilistic model, which combines a *prior* distribution over a set of parameters with the data to draw samples from *posterior* distribution over the parameters using Hamiltonian Markov Chain Monte Carlo.
\item Check for sampling convergence.
\item Use prior and posterior predictive checks to evaluate the model specification and fit.
\item Use cross validation to evaluate the generalizability of the model.
\item Assess inferences that can be made from the model.
\end{enumerate}

# BayesPharma package design {#package_design}
We designed the BayesPharma package to support modeling of foundational pharmacology models using the principle Bayesian workflow. As described above, the workflow involves four phases: model specification, model fitting, model evaluation, and interpretation. Here we will describe the BayesPharma API and how we recommend using it. 

To provide data to the BayesPharma package, the user provides a R \texttt{data.frame} with columns \texttt{response}, \mbox{log\_dose}, optionally additional covariates such as \texttt{drug\_id} or \texttt{batch\_id}. To specify the model involves specifying the (1) the formulate relating the predictors to the response, (2) the family (3) priors over the model parameters (4) initial values, and (5) additional arguments to tune sampling.

# Case Studies
In this section we will consider several models as case studies: the sigmoidal hill model in #{sec:hill}, the MuSyC synergy model in {#sec:MuSyC}, Michaelis-Menten enzyme progress curve in #{sec:michaelis_menten}. For each we will implement it, and apply it to example data by fitting different models and then we will compare the models based on their fit of the data and inferences that can be made.

## Hill Equation {#hill}
In this case study we are going to reanalyze the dose response of 4 Kappa Opioid 
receptor (KOR) antagonists using the `BayesPharma` package from from a study 
done by Margolis et al. (-@Margolis2020-bm). Whole cell electrophysiology in acute rat midbrain slices was used to evaluate pharmacological properties of four novel KOR antagonists: BTRX-335140, BTRX-395750, PF-04455242, and JNJ-67953964

Originally paper, the dose-response data analysis was done by using the `drc` package in R which implements the minimization of negative log likelihood function and reduces to least square estimation for a continuous response. The data was normalized to % baseline then fit to a 4-parameter log-logistic dose response model, setting the top (max response) to 100% and estimating the IC50, its variance, and the bottom (min response).

```{r kor-model-prep}
load("../data/kor_antag.rda")
kor_antag <- kor_antag |> dplyr::transmute(
  substance_id = Drug,
  cell_id = cell_number,
  log_dose = log_dose,
  response = normalized_measurement)
```

```{r kor-model, eval=FALSE}
#| echo=TRUE,
#| results='hide',
#| message=FALSE,
#| dependson = c("kor-model-prep")
kor_model <- BayesPharma::dr_model(
  data = kor_antag |> dplyr::select(substance_id, log_dose, response),
  formula = BayesPharma::dr_formula(predictors = 0 + substance_id), 
  priors = BayesPharma::dr_priors(inhibitor = TRUE, top = 100),
  init = BayesPharma::dr_inits(),
  backend="cmdstanr")
```




## MuSyC synergy model} {#MuSyC}
When two independent treatments are combined they may synergize to effect the response. A typical use case is combining to different drugs to determine if there is signaling epistasis or to assess the value of combining them as a combination therapy. Alternatively for ion channels, a drug and voltage can be combined to evaluate the effect on the membrane potential.

When two treatments are combined they can influence each other in different ways. The combine treatment can elicit a stronger or weaker effect than either treatment separately, or in the presence of one drug, the the dose at which the the other effects the outcome may be shifted. These different forms of synergy are modeled by the classical Bliss and Loewe models of synergy, respectively. Recently XXX derived the MuSyC model that combines these different forms of synergy.  

The functional form for the MuSyC model gives an equation for the response $\color{response}{E_d}$ at doses of $\color{dose}{d_1}$ and $\color{dose}{d_2}$ of the two treatments and has $9$ free parameters $\{\color{dose}{C_1}, \color{dose}{C_2}, \color{response}{E_0}, \color{response}{E_1}, \color{response}{E_2}, \color{response}{E_3}, \color{shape}{h_1}, \color{shape}{h_2}, \color{shape}{\alpha}\}$:

\begin{align}
\color{response}{E_d} &= \frac{
          {\color{dose}{C_1}}^{\color{shape}{h_1}}{\color{dose}{C_2}}^{\color{shape}{h_2}}{\color{response}{E_0}} +
          {\color{dose}{d_1}}^{\color{shape}{h_1}}{\color{dose}{C_2}}^{\color{shape}{h_2}}{\color{response}{E_1}} +
          {\color{dose}{C_1}}^{\color{shape}{h_1}}{\color{dose}{d_2}}^{\color{shape}{h_2}}{\color{response}{E_2}} +
          {\color{dose}{d_1}}^{\color{shape}{h_1}}{\color{dose}{d_2}}^{\color{shape}{h_2}}{\color{shape}{\alpha}} {\color{response}{E_3}}
       }{
          {\color{dose}{C_1}}^{\color{shape}{h_1}}{\color{dose}{C_2}}^{\color{shape}{h_2}} +
          {\color{dose}{d_1}}^{\color{shape}{h_1}}{\color{dose}{C_2}}^{\color{shape}{h_2}} +
          {\color{dose}{C_1}}^{\color{shape}{h_1}}{\color{dose}{d_2}}^{\color{shape}{h_2}} +
          {\color{dose}{d_1}}^{\color{shape}{h_1}}{\color{dose}{d_2}}^{\color{shape}{h_2}}{\color{shape}{\alpha}}}
\end{align}

To interpret these parameters if we set $\color{dose}{d_2}=0$, then 
\begin{align}
\color{response}{E_d} &= \frac{
          {\color{dose}{C_1}}^{\color{shape}{h_1}}{\color{response}{E_0}} +
          {\color{dose}{d_1}}^{\color{shape}{h_1}}{\color{response}{E_1}}
       }{
          {\color{dose}{C_1}}^{\color{shape}{h_1}} +
          {\color{dose}{d_1}}^{\color{shape}{h_1}}}
\end{align}
which is the Hill equation, which we modeled above \ref{sec:hill}. If we then additionally set $\color{dose}{d_1}=0$ then $\color{response}{E_d}=\color{response}{E_0}$, in the limit as ${\color{dose}{d_1}}\rightarrow \infty$ then ${\color{response}{E_d}}\rightarrow {\color{response}{E_1}}$, and if ${\color{dose}{d_1}}=\color{dose}{C_1}$ then ${\color{response}{E_d}} = ({\color{response}{E_0}} + {\color{response}{E_2}})/2$, which is the half maximal response (either the $\color{response}{\mbox{IC}_{50}}$ if treatment $1$ is an inhibitor or $\color{response}{\mbox{EC}_{50}}$ if treatment $1$ is agonist). The slope at ${\color{dose}{d_1}}={\color{dose}{C_1}}$ is 
\begin{align*}
    \frac{\mathrm{d}\;\color{response}{E_d}}{\mathrm{d}\color{dose}{d_1}}
        &= {\color{dose}{C_1}}^{v}{\color{response}{E_0}}
              \frac{\mathrm{d}}{\mathrm{d}\color{dose}{d_1}}
                  \frac{1}{{\color{dose}{C_1}}^{\color{shape}{h_1}} + {\color{dose}{d_1}}^{\color{shape}{h_1}}} +
           {\color{response}{E_1}}
              \frac{\mathrm{d}}{\mathrm{d}\color{dose}{d_1}}
                   \frac{{\color{dose}{d_1}}^{h_1}}{{\color{dose}{C_1}}^{\color{shape}{h_1}} + {\color{dose}{d_1}}^{\color{shape}{h_1}}}\\
        &= {\color{dose}{C_1}}^{h_1}{\color{response}{E_0}}
              \frac{                          h_1{\color{dose}{d_1}}^{{\color{shape}{h_1}}-1}}{\left({\color{dose}{C_1}}^{\color{shape}{h_1}} + {\color{dose}{d_1}}^{\color{shape}{h_1}}\right)^2} +
            {\color{response}{E_1}}
               \frac{{\color{dose}{C_1}}^{\color{shape}{h_1}}h_1{\color{dose}{d_1}}^{{\color{shape}{h_1}}-1}}{\left({\color{dose}{C_1}}^{\color{shape}{h_1}} + {\color{dose}{d_1}}^{\color{shape}{h_1}}\right)^2}\\
        &= ({\color{response}{E_0}} + {\color{response}{E_1}})
\end{align*}

The evaluation of the functional form for ${\color{response}{E_d}}$ is numerically unstable. To transform using the $\mbox{log\_sum\_exp}$ trick, let

\begin{align*}
\mbox{numerator\_parts} = [\\
      &{\color{shape}{h_1}}\log({\color{dose}{C_1}}) + {\color{shape}{h_2}}\log({\color{dose}{C_2}}) + \log({\color{response}{E_0}}),\\
      &{\color{shape}{h_1}}\log({\color{dose}{d_1}}) + {\color{shape}{h_2}}\log({\color{dose}{C_2}}) + \log({\color{response}{E_1}}),\\
      &{\color{shape}{h_1}}\log({\color{dose}{C_1}}) + {\color{shape}{h_2}}\log({\color{dose}{d_2}}) + \log({\color{response}{E_2}}),\\
      &{\color{shape}{h_1}}\log({\color{dose}{d_1}}) + {\color{shape}{h_2}}\log({\color{dose}{d_2}}) + \log({\color{response}{E_3}}) + \log({\color{shape}{\alpha}}) ]\\
\mbox{denominator\_parts} = [\\
      &{\color{shape}{h_1}}\log({\color{dose}{C_1}}) + {\color{shape}{h_2}}\log({\color{dose}{C_2}}),\\
      &{\color{shape}{h_1}}\log({\color{dose}{d_1}}) + {\color{shape}{h_2}}\log({\color{dose}{C_2}}),\\
      &{\color{shape}{h_1}}\log({\color{dose}{C_1}}) + {\color{shape}{h_2}}\log({\color{dose}{d_2}}),\\
      &{\color{shape}{h_1}}\log({\color{dose}{d_1}}) + {\color{shape}{h_2}}\log({\color{dose}{d_2}})]\\
\end{align*}
Then 
$$
    E_d = \mbox{exp}\!\left(\mbox{log\_sum\_exp}(\mbox{numerator\_parts}) - \mbox{log\_sum\_exp}(\mbox{denominator\_parts})\right).
$$

## Michaelis-Menten enzyme progress model {#michaelis_menten}

## Model selection {#model_selection}

(Vehtari, 2016, https://arxiv.org/abs/1507.04544)
predictive accuracy
   how well would the model generalize to new data?
   empirical risk minimization of the model log-likelihood
   leave-k-out cross validation
      - expensive to re-fit model k times
      - approximation via importance sampling but may be unstable
      - Pareto smoothed importance sampling (PSIS) more stable



## Over dispersed negative binomial response

The response of an assay results from a measurement of the experimental system. Often the measurements are normalized so that the response for negative control is 1 (e.g. diseased) and the positive control is 0 (e.g. healthy). However when the robustness of the measurement depends on the measured value, this normalization can make it difficult to combine different measurements. An alternative approach is to model the measurements directly, to take into account the uncertainty associated with the response. To illustrate, if the measurement is the relative number of cells having a phenotype, then five out of ten cells and five thousand out of ten thousand cells will have the same response of 0.5, but the former will a less reliable measurement.        

